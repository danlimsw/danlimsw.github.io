{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb2b901-cfdf-4112-bc23-5f96afde9692",
   "metadata": {},
   "source": [
    "# An example-driven introduction to automatic differentiation\n",
    "Author: Daniel Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a442417-9f83-4b64-bbe7-24b5873954ed",
   "metadata": {},
   "source": [
    "## 0. Package installation\n",
    "Make sure you have started a new environment and installed the necessary packages; for example: \n",
    "```{bash}\n",
    "conda create --name autodiff python=3.9\n",
    "y\n",
    "conda activate autodiff\n",
    "pip install --upgrade pip\n",
    "pip install --upgrade numpy autograd notebook\n",
    "pip install --upgrade matplotlib scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fb045-acb3-4f9a-b8d5-ba1c2627b8b9",
   "metadata": {},
   "source": [
    "## 1. Quick introduction to Autograd\n",
    "Autograd is a simple Python package that performs automatic differentiation. It is a wrapper for Numpy - that means, Autograd \"replaces\" Numpy in your calculations and gives the same answers, but very importantly, gives access to efficient gradients. \n",
    "\n",
    "This might sound unfamiliar. Let's take a simple case and use Autograd to calculate the gradient of $y=x^2$. We expect $dy/dx=2x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99909cb9-add9-4c64-9e46-9ac1bbd72743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "# define the function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# get the gradient function\n",
    "grad_f = grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acbe1e-9232-45de-a1fb-bd6a1e109948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this on various values\n",
    "print(f\"At x=0.0, f(x)={f(0.0)}, and f'(x)={grad_f(0.0)}\")\n",
    "print(f\"At x=1.0, f(x)={f(1.0)}, and f'(x)={grad_f(1.0)}\")\n",
    "print(f\"At x=-2.0, f(x)={f(-2.0)}, and f'(x)={grad_f(-2.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d8044-2f60-449f-a455-868ce491bf30",
   "metadata": {},
   "source": [
    "It works! Let's extend this to several dimensions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc109b8-b060-43f6-97af-8efa5755feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def f(x):\n",
    "    return x[0]*x[1]*x[2]\n",
    "\n",
    "# get the gradient function\n",
    "grad_f = grad(f)\n",
    "\n",
    "# test this on various values\n",
    "print(f\"At x=[0.0,1.0,2.0], f(x)={f([0.0,1.0,2.0])}, and f'(x)={grad_f([0.0,1.0,2.0])}\")\n",
    "# print(f\"At x=[0.0,0.0,0.0], f(x)={f([0.0,0.0,0.0])}, and f'(x)={grad_f([0.0,0.0,0.0])}\")\n",
    "# print(f\"At x=[10.0,20.0,30.0], f(x)={f([10.0,20.0,30.0])}, and f'(x)={grad_f([10.0,20.0,30.0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f0b50-2a23-4b21-aba1-c038b4debb94",
   "metadata": {},
   "source": [
    "Note that we need to supply a three-element array $x$ to both f and grad_f. grad_f returns a 3-element array as the 3D gradient - the gradient of f in each of the three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8f4bf-eecd-40df-a953-53c8d307fe11",
   "metadata": {},
   "source": [
    "## 2. Timing demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb786c6-5998-430c-8fd2-736374a9ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import time\n",
    "\n",
    "# define the function\n",
    "def f(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "# get the gradient function\n",
    "grad_f = grad(f)\n",
    "\n",
    "N_dims = 1000000 # number of tunable parameters\n",
    "N_iter = 100 # number of iterations to run for a timing run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4840931-e0d2-4bee-a167-aeceb6fd9263",
   "metadata": {},
   "source": [
    "Let's time how long it takes to run the forward pass (just computing $f$ for various x-positions) for N_iter times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85d7d9-4abb-4380-bbbd-50f2d60c3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for j in range(N_iter):\n",
    "    x0 = np.random.rand(N_dims) # position to evaluate the function and gradient at\n",
    "    f_val = f(x0)\n",
    "end_time_single_pass = time.time()\n",
    "duration_single_pass = end_time_single_pass - start_time\n",
    "print(f\"Time taken for a {N_iter} forward passes: {(duration_single_pass):.6f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2d4f7-f14f-47e0-bea5-f6b5c11c117d",
   "metadata": {},
   "source": [
    "We will do the same timing for the gradient calculation for N_iter times. Note that the output of each calculation is a N_dims-long vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd5b77-d387-48ef-9df0-38b7a64d3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for j in range(N_iter):\n",
    "    x0 = np.random.rand(N_dims) # position to evaluate the function and gradient at\n",
    "    grad_f_val = grad_f(x0)\n",
    "end_time_grad = time.time()\n",
    "duration_grad = end_time_grad - start_time\n",
    "print(f\"Time taken for {N_iter} gradient calculations: {(duration_grad):.6f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22457d40-5f00-4fb8-b85c-c9c88d047a6b",
   "metadata": {},
   "source": [
    "Let's see how much longer it took to compute the gradient relative to the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10aa535-805e-4ff9-9d40-14f9268d1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ratio of gradient calculation time to forward pass time: {(duration_grad/duration_single_pass):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6f693-87e3-4c54-b415-0c58459e6773",
   "metadata": {},
   "source": [
    "## 3. Brachistochrone curve calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47949e8d-baa5-46c1-a221-cd7c0e30d50c",
   "metadata": {},
   "source": [
    "We will now numerically approximate the Brachistochrone curve. The Brachistochrone curve is the smooth curve connecting two points so that a particle will slide under gravity from one point to another in the shortest time possible. Contrary to expectations, it is not a straight line connecting these two points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dceb51-37e2-405b-9b37-1d07241d4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "#---------------------------\n",
    "# GEOMETRY SETUP\n",
    "#---------------------------\n",
    "\n",
    "g = 9.81 # [m/s^2] gravitational acceleration\n",
    "x_start = 0.0 # [m]\n",
    "x_end = 1.0 # [m]\n",
    "y_start = 1.0 # [m]\n",
    "y_end = 0.0 # [m]\n",
    "N = 25 # total number of discretization points (excluding endpoints)\n",
    "\n",
    "# the y-positions will be sampled uniformly from top to bottom\n",
    "y = np.linspace(y_start, y_end, N+2)\n",
    "\n",
    "#---------------------------\n",
    "# FUNCTION AND GRADIENT SETUP\n",
    "#---------------------------\n",
    "\n",
    "def construct_x(x_interior):\n",
    "    \"\"\"\n",
    "    Reconstruct the full x-array by including boundary x-values.\n",
    "    \"\"\"\n",
    "    return np.concatenate(([x_start], x_interior, [x_end]))\n",
    "\n",
    "def total_time(x_interior):\n",
    "    \"\"\"\n",
    "    Computes the total descent time along the discretized curve.\n",
    "    The time for each segment is approximated by:\n",
    "        Δt = ds / sqrt(2*g*(y_start - y_avg))\n",
    "    where ds is the segment length and y_avg is the average y-value of the segment.\n",
    "    \"\"\"\n",
    "    x_full = construct_x(x_interior)\n",
    "    T = 0.0\n",
    "    for i in range(N + 1):\n",
    "        dx = x_full[i+1] - x_full[i]\n",
    "        dy = y[i+1] - y[i]\n",
    "        ds = np.sqrt(dx**2 + dy**2)\n",
    "        y_avg = (y[i] + y[i+1]) / 2.0\n",
    "        T += ds / np.sqrt(2 * g * (y_start - y_avg))\n",
    "    return T\n",
    "\n",
    "# Get the gradient function using autograd.\n",
    "grad_total_time = grad(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ea870-49c5-442f-85d7-a09cd67c6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# OPTIMIZATION SETUP\n",
    "#---------------------------\n",
    "\n",
    "# define a starting guess: a straight line connecting the two endpoints\n",
    "x_interior = np.linspace(x_start, x_end, N+2)[1:-1] #  we remove the first and last endpoints since they are not being optimized\n",
    "\n",
    "# specify the gradient descent parameters\n",
    "learning_rate = 2e-2 # size of each gradient descent step\n",
    "num_iters = 5000 # maximum number of gradient descent iterations\n",
    "obj_history = [] # we will record the performance at each optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a9795-786e-45b4-abb7-678d6455c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# RUN OPTIMIZATION\n",
    "#---------------------------\n",
    "# Optimize via gradient descent.\n",
    "for i in range(num_iters):\n",
    "    grad_val = grad_total_time(x_interior)\n",
    "    x_interior = x_interior - learning_rate * grad_val\n",
    "    obj_history.append(total_time(x_interior))\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Iteration {i}, total time for particle to slide down: {total_time(x_interior):.4f} seconds\")\n",
    "\n",
    "print(f\"Optimized total time for particle to slide down: {total_time(x_interior)} seconds.\")\n",
    "x_opt = construct_x(x_interior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba32e5f-fdfd-4704-98e4-3b6ede12d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# PLOTS\n",
    "#---------------------------\n",
    "# 1) Plot the convergence of the total descent time.\n",
    "plt.figure()\n",
    "plt.plot(obj_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Total descent time [s]\")\n",
    "plt.title(\"Particle travel time against iteration\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Overlay the optimized curve\n",
    "plt.figure()\n",
    "plt.plot(x_opt, y, 'b.-')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimized Brachistochrone curve')\n",
    "plt.axis('equal')\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1e2ac-cf41-4c11-8b62-e23662f36411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# THEORETICAL CYCLOID CALCULATION\n",
    "#---------------------------\n",
    "#\n",
    "# The theoretical brachistochrone (for a particle sliding from (0,1) to (1,0))\n",
    "# is a cycloid, described parametrically by:\n",
    "#\n",
    "#   x(θ) = (R/2) * (θ - sin θ)\n",
    "#   y(θ) = 1 - (R/2) * (1 - cos θ)\n",
    "#\n",
    "# The boundary conditions x(θ_f)=1 and y(θ_f)=0 imply:\n",
    "#   1 - (R/2) * (1 - cos θ_f) = 0  =>  R = 2/(1-cos θ_f)\n",
    "#   (R/2) * (θ_f - sin θ_f) = 1\n",
    "#\n",
    "# Substituting R, the equation to solve for θ_f becomes:\n",
    "#   θ_f - sin θ_f = 1 - cos θ_f\n",
    "\n",
    "def equation(theta):\n",
    "    \"\"\"\n",
    "    Residual for the transcendental equation: theta - sin(theta) - (1 - cos(theta)) = 0.\n",
    "    \"\"\"\n",
    "    return theta - np.sin(theta) - (1 - np.cos(theta))\n",
    "\n",
    "# Use fsolve to compute θ_f; initial guess of 2.0.\n",
    "theta_f_guess = 2.0\n",
    "theta_f_solution = fsolve(equation, theta_f_guess)[0]\n",
    "R_solution = 2 / (1 - np.cos(theta_f_solution))\n",
    "\n",
    "print(f\"Theoretical solution: theta_f = {theta_f_solution:.6f}, R = {R_solution:.6f}\")\n",
    "\n",
    "# Generate the theoretical cycloid curve.\n",
    "theta_vals = np.linspace(0, theta_f_solution, 200)\n",
    "x_cycloid = (R_solution / 2.0) * (theta_vals - np.sin(theta_vals))\n",
    "y_cycloid = 1.0 - (R_solution / 2.0) * (1.0 - np.cos(theta_vals))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_opt, y, 'b.-', label='Optimized (Discretized) Curve')\n",
    "plt.plot(x_cycloid, y_cycloid, 'r--', label='Theoretical Cycloid')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimized Brachistochrone vs. Theoretical Cycloid')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66a1d8-aaca-429e-8d98-9a20fb58c8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
